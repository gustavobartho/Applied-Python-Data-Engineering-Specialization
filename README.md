# Applied Python Data Engineering Specialization

Specialization Link: https://www.coursera.org/programs/universidade-de-sao-paulo-br-on-coursera-mvxtw/specializations/python-data-engineering

Learn how to use data engineering to leverage big data for business strategy, data analysis, or machine learning and AI. By completing this course series, you'll empower yourself with the knowledge and proficiency required to build efficient data pipelines, manage cutting-edge platforms like Hadoop, Spark, Snowflake, Databricks, and Kubernetes, and tell stories with data through visualization. You will delve into foundational big data concepts, distributed computing with Spark, Snowflake’s architecture, Databricks’ machine learning capabilities, Python techniques for data visualization, and critical methodologies like DataOps. 

This course series is designed for software engineers, developers, researchers, and data scientists who want to strengthen their specialization in data science or machine learning, as well as for professionals who are interested in pursuing a career as a data-focused software engineer, data scientist, or a data engineer working in cloud, machine learning, business intelligence, or other field. 

Applied Learning Project

The Specialization features a capstone project focused on using Databricks’ API to replicate an existing project. This provides hands-on experience working with Databricks to build a portfolio-ready data solution. You will apply Python to a variety of data engineering tasks.

---
---

## Module 1 - Spark, Hadoop, and Snowflake for Data Engineering

Course Link: https://www.coursera.org/programs/universidade-de-sao-paulo-br-on-coursera-mvxtw/learn/spark-hadoop-snowflake-data-engineering

This is primarily aimed at first- and second-year undergraduates interested in engineering or science, along with high school students and professionals with an interest in programmingGain the skills for building efficient and scalable data pipelines. Explore essential data engineering platforms (Hadoop, Spark, and Snowflake) as well as learn how to optimize and manage them. Delve into Databricks, a powerful platform for executing data analytics and machine learning tasks, while honing your Python data science skills with PySpark. Finally, discover the key concepts of MLflow, an open-source platform for managing the end-to-end machine learning lifecycle, and learn how to integrate it with Databricks.

This course is designed for learners who want to pursue or advance their career in data science or data engineering, or for software developers or engineers who want to grow their data management skill set. In addition to the technologies you will learn, you will also gain methodologies to help you hone your project management and workflow skills for data engineering, including applying Kaizen, DevOps, and Data Ops methodologies and best practices.

#### This Module main goal are:
* Create scalable data pipelines (Hadoop, Spark, Snowflake, Databricks) for efficient data handling.
* Optimize data engineering with clustering and scaling to boost performance and resource use.
* Build ML solutions (PySpark, MLFlow) on Databricks for seamless model development and deployment.
* Implement DataOps and DevOps practices for continuous integration and deployment (CI/CD) of data-driven applications, including automating processes.

---

### Week 1 - Overview and Introduction to PySpark
In this module, you will learn how to work with different data engineering platforms, such as Hadoop and Spark, and apply their concepts to real-world scenarios. First, you will explore the fundamentals of Hadoop to store and process big data. Next, you will delve into Spark concepts, distributed computing, deferred execution, and Spark SQL. By the end of the week, you will gain hands-on experience with PySpark DataFrames, DataFrame methods, and deferred execution strategies.

#### Week learning objectives:
* Describe the challenges of building efficient and scalable data pipelines.
* Apply strategies for ingesting and storing data in Hadoop.
* Integrate Spark into Hadoop pipelines for data transformation and processing.

---

### Week 2 - Snowflake
In this module, you will explore the Snowflake platform, gaining insights into its architecture and key concepts. Through hands-on practice in the Snowflake Web UI, you'll learn to create tables, manage warehouses, and use the Snowflake Python Connector to interact with tables. By the end of this week, you'll solidify your understanding of Snowflake's architecture and practical applications, emerging with the ability to effectively navigate and leverage the platform for data management and analysis.

#### Week learning objectives:
* Load sample data into Snowflake tables from local files or cloud storage.
* Sign up for a Snowflake account and create a database and schema.
* Write SQL queries to retrieve and analyze data from Snowflake tables.

---

### Week 3 - Azure Databricks and MLFLow
In this module, you will practice the essential skills for seamlessly managing machine learning workflows using Databricks and MLFlow. First, you will create a Databricks workspace and configure a cluster, setting the stage for efficient data analysis. Next, you will load a sample dataset into the Databricks workspace using the power of PySpark, enabling data manipulation and exploration. Finally, you will install MLFlow either locally or within the Databricks environment, gaining the ability to orchestrate the entire machine learning lifecycle. By the end of this week, you will be able to craft, track, and manage machine learning experiments within Databricks, ensuring precision, reproducibility, and optimal decision-making throughout your data-driven journey.

#### Week learning objectives:
* Create a new Databricks workspace and configure a cluster.
* Load a sample dataset into the Databricks workspace using PySpark.
* Install MLFlow on your local machine or in your Databricks workspace.
* Use MLFlow to create, track, and manage machine learning experiments within Databricks.

---

### Week 4 - DataOps and Operations Methodologies
In this module, you will explore the concepts of Kaizen, DevOps, and DataOps and how these methodologies synergistically contribute to efficient and seamless data engineering workflows. Through practical examples, you will learn how Kaizen's continuous improvement philosophy, DevOps' collaborative practices, and DataOps' focus on data quality and integration converge to enhance the development, deployment, and management of data engineering platforms. By the end of this week, you will have the knowledge and perspective needed to optimize data engineering processes and deliver scalable, reliable, and high-quality solutions.

#### Week learning objectives:
* Review Kaizen, DevOps, and DataOps methodologies and their significance in data engineering platforms.
* Develop a data engineering solution with a minimal and essential subset of the Python language and the Linux environment.

---
---
