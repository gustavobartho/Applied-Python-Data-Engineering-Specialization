{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SPARK_HOME'] = \"/home/user/.application-data/spark\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'notebook'\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'\n",
    "\n",
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the department\n",
    "department1 = Row(id='123456', name='Computer Science')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the employee table schema and an employee\n",
    "Employee = Row('first_name', 'last_name', 'email', 'salary')\n",
    "employee1 = Employee('Michael', 'Jackson', 'michael.jackson@rusbe.com', 1000000)\n",
    "employee2 = Employee('Manoel', 'Gome', 'manoel.gome@bluepen.com', 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the DepartmentWithEmployees instance from Department and Employees\n",
    "departmentWithEmployees = Row(department=department1, employees = [employee1, employee2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/20 15:11:48 WARN Utils: Your hostname, user-PC resolves to a loopback address: 127.0.1.1; using 192.168.0.27 instead (on interface wlp3s0)\n",
      "24/09/20 15:11:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/20 15:11:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+-----------+----------+-------+\n",
      "|    account_number|      aba|        bic|    opened|balance|\n",
      "+------------------+---------+-----------+----------+-------+\n",
      "|CBXD44669851029839|102317138|RWWEGBRO0T5|2009-06-04| 285769|\n",
      "|GMHS11683964344077|082341703|   TIDCGB2I|2014-06-26| 591000|\n",
      "|GCQT83212037197512|107809939|HGMJGB9AIFY|2006-08-07|1631126|\n",
      "|TDDI73474064461675|046709967|VQEAGBT2HP7|2001-03-20|1977359|\n",
      "|HDVG29919274654139|069217423|BQCSGBADD0Z|2014-11-05| 208204|\n",
      "+------------------+---------+-----------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# join dataframes (concatenate)\n",
    "spark = SparkSession.builder.appName('pyspark-intro').getOrCreate()\n",
    "accounts = spark.read.option('header', True).csv('../data/accounts.csv')\n",
    "accounts_dup = accounts.union(accounts)\n",
    "display(accounts_dup.show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------+\n",
      "|    account_number|balance squared|\n",
      "+------------------+---------------+\n",
      "|CBXD44669851029839|           NULL|\n",
      "|GMHS11683964344077|           NULL|\n",
      "|GCQT83212037197512|           NULL|\n",
      "|TDDI73474064461675|           NULL|\n",
      "|HDVG29919274654139|           NULL|\n",
      "+------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# UDF (User Defined Functions)\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "def squared(x):\n",
    "    return float(x) * float(x)\n",
    "\n",
    "spark.udf.register('squared', squared)\n",
    "squared_udf = udf(squared, LongType())\n",
    "\n",
    "df_sqrd = accounts.select('account_number', squared_udf('balance').alias('balance squared'))\n",
    "display(df_sqrd.show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/20 15:12:02 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "/home/user/.application-data/miniconda3/envs/data-eng/lib/python3.12/site-packages/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n",
      "/home/user/.application-data/miniconda3/envs/data-eng/lib/python3.12/site-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id\n",
       "0   0\n",
       "1   1\n",
       "2   2\n",
       "3   3\n",
       "4   4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pyspark pandas API\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "ps_df = ps.range(10)\n",
    "pd_df = ps_df.to_pandas()\n",
    "pd_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-eng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
